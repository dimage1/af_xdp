DONE

	Sketch out client and server programs

	Update makefile and get it building and linking on linux

	Read AF_XDP resources:

		https://medium.com/high-performance-network-programming/recapitulating-af-xdp-ef6c1ebead8				<--- really good
		https://www.kernel.org/doc/html/latest/networking/af_xdp.html
		https://github.com/xdp-project/xdp-tutorial/tree/master/advanced03-AF_XDP
		https://github.com/gamemann/XDP-Stats/tree/master/src/af_xdp
		https://github.com/xdp-project/bpf-examples/tree/master/AF_XDP-forwarding
		https://networkbuilders.intel.com/docs/networkbuilders/af-xdp-sockets-high-performance-networking-for-cloud-native-networking-technology-guide.pdf

	Find the name of the interface on hulk for the 10G NIC... 

		enp8s0f0

	Get the reflector server running on hulk linux box with the 10G NIC. Disable logging so it's performant.

	Set to only one RSS queue for now:

		sudo ethtool -L enp8s0f0 combined 1
		sudo ethtool -l enp8s0f0

	Setup vision linux box so it's ready to go

	Modify client so it has hard coded interface name

	Get the server IP address and port hardcoded in the client

		192.168.183.124:40000

	Sketch out the client as required to work with AF_XDP packets

	Following tutorial here: https://github.com/xdp-project/xdp-tutorial/tree/master/advanced03-AF_XDP

	Packet buffer is created.

	umem is created.

	Cleanup for buffer and umem.

	Create the xsk map in the xdp program and grab file handle in userspace

	Create the xsk socket

	Clean up the xsk socket on destroy

	Direct RSS to queue 0

		sudo ethtool -N enp8s0f0 flow-type udp4 action 0

    Wrap my head around exactly what is going on with the ring buffers

    The example code shows a system which processes received packets, and if valid, responds 1:1 with a packet.

    As such it's a system driven by packet receives, it reuses the frame for the received packet when it sends a response.

    My system needs to be different, it's about sending packets. The actual receives could be done inside the xdp program (and probably should)

    In a future article I could write an example that combines sending and receiving.

    This example would just demonstrate how to do the sends?

    The receives are done in batches, which seems like a concern when the system may only have one client connected to it: RX_BATCH_SIZE

	Study the simple allocator. What exactly are they doing? It seems overly simplistic...

	If I'm not allowing receives through the system, then I can drive by sends -- this could simplify the allocation of frames?

	arp command on linux to get arp table (address resolution protocol, eg. get ethernet address for given ip address)

	https://www.geeksforgeeks.org/arp-command-in-linux-with-examples/?ref=lbp

	Work out strategy to send packets. 

	I think it can just be a rolling index modulo MAX_FRAMES?

	Stop if the current frame is not completed yet.

	Need to implement completion for sends to mark frames as free to use for send.

	Doesn't seem hard.

	Actually, it seems that the way the stuff works is that you have an array of free frames.

	You setup these free frames initially, and when you have sends complete, you add the frame back into the free array.

	I think the way the APIs work, they require passing in the free frames array -- otherwise, I'd use the rolling index

	Double check this...

	This code does the simple array of free frames

		static uint64_t xsk_alloc_umem_frame(struct xsk_socket_info *xsk)
		{
		    uint64_t frame;
		    if (xsk->umem_frame_free == 0)
		        return INVALID_UMEM_FRAME;

		    frame = xsk->umem_frame_addr[--xsk->umem_frame_free];
		    xsk->umem_frame_addr[xsk->umem_frame_free] = INVALID_UMEM_FRAME;
		    return frame;
		}

		static void xsk_free_umem_frame(struct xsk_socket_info *xsk, uint64_t frame)
		{
		    assert(xsk->umem_frame_free < NUM_FRAMES);

		    xsk->umem_frame_addr[xsk->umem_frame_free++] = frame;
		}

		static uint64_t xsk_umem_free_frames(struct xsk_socket_info *xsk)
		{
		    return xsk->umem_frame_free;
		}

	It's simple enough. Adapted it and moving on.

	Get everything compiling, linking and running.

	Actually construct the packet to send.

	Construct ethernet header

	Construct ip header

	Construct udp header

	Get ipv4 checksum working

	Pin client to run on core 0.

	Instead of constructing a single packet, I should construct a batch of packets to send, eg. grab all the free frames for send, iterate across, and gen them in one go

	And then make one syscall to send them

	Set the batch size to be configurable. Default it to 256.
		
	Verify that the server receives packets sent from the client.

	Verify that the client receives reflected packets back from the server

	Number of packets sent on client (drive from AF_XDP) -- atomic

	Number of packets received on client (drive from XDP program) -- atomic

	Create a stats thread

	Print out stats once per-second on stats thread.

	I can send more than 1 million packets per-second from a single core on vision. Pretty amazing!

	Debug why not seeing packets back from server

	Server is receiving packets from the client

	It thinks it is reflecting them

	The client isn't logging any packets received back from server -- but this used to be working. What's going on?

	It looks like the interface isn't getting any packets sent back to it from the server. What's wrong?

	ifconfig

		enp8s0f0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
	        inet 192.168.183.121  netmask 255.255.255.0  broadcast 192.168.183.255
	        inet6 fe80::7a32:17d5:df24:24fd  prefixlen 64  scopeid 0x20<link>
	        ether a0:36:9f:68:eb:98  txqueuelen 1000  (Ethernet)
	        RX packets 0  bytes 0 (0.0 B)
	        RX errors 239  dropped 0  overruns 0  frame 239
	        TX packets 346637  bytes 49221386 (49.2 MB)
	        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

	Could it be because I don't have the receive queues setup? I remember it used to work, but this was before I simplified.

	Don't think so.

	Try dropping send batch size to 1...

	Remove reflection. It's not necessary for this example.

	Put received packet stats on the server side.

	Try increasing txqueuelen:

	   sudo ifconfig enp8s0f0 mtu 1500 txqueuelen 10000 up

	I think we need to spread the load so packets are received on multiple RSS queues (cores) on hulk

	Convert stats to accum version so we can see sent and received per-second.

	I could lie about the logs from UDP address when sending packets, so it gets spread across multiple receive queues... but it seems that I don't need to bother. A single core can do 6million packets per-second.

	Create a 001 and 002

	Update readme for 002 and call it out.

	Disable hyperthreading.

	Modify the code to have per-socket data.

	Modify the code to schmear an increasing counter per-send thread into the source address.

	Implement socket threads and run it with 2 CPUs.

	Verify that packets are received on the server side up to 12.2M packets per-second @ 100 byte UDP packets.

	Can't quite get up to 12.2M packets. It's less.

	Need to noodle around...

	Goal should be to hit line rate @ 60 byte packets (with 4 byte frame check sequence)

	https://www.fmad.io/blog/what-is-10g-line-rate

	Try using need wakeup XDP flag, and it does improve things

TODO
	
	It looks on the client side that only 8 cores are working for sends, even though I have 32 cores that should be sending packets. Is this the bottleneck?

		glenn@vision:~$ top

		top - 11:19:43 up 3 days, 22:45,  4 users,  load average: 33.58, 15.51, 18.62
		Tasks: 858 total,   9 running, 849 sleeping,   0 stopped,   0 zombie
		%Cpu(s):  0.0 us,  0.3 sy, 79.9 ni,  0.0 id,  0.0 wa,  0.0 hi, 19.8 si,  0.0 st
		MiB Mem :  64256.0 total,  57773.8 free,   4429.9 used,   2052.3 buff/cache
		MiB Swap:  20479.5 total,  20479.5 free,      0.0 used.  56411.4 avail Mem

		    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
		  31687 root      26   6  801864 529920   4352 S  2568   0.8  15:19.50 client
		     15 root      20   0       0      0      0 R  79.1   0.0 483:13.15 ksoftirqd/0
		     25 root      20   0       0      0      0 R  79.1   0.0   5170:11 ksoftirqd/1
		     31 root      20   0       0      0      0 R  79.1   0.0 495:56.73 ksoftirqd/2
		     43 root      20   0       0      0      0 R  79.1   0.0 480:38.81 ksoftirqd/4
		     37 root      20   0       0      0      0 R  78.8   0.0 483:12.97 ksoftirqd/3
		     49 root      20   0       0      0      0 R  78.8   0.0 480:23.82 ksoftirqd/5
		     55 root      20   0       0      0      0 R  78.8   0.0 483:30.50 ksoftirqd/6
		     61 root      20   0       0      0      0 R  78.8   0.0 483:19.91 ksoftirqd/7
		  31721 glenn     26   6   23852   4864   3328 R   0.7   0.0   0:00.17 top
		  26527 root      20   0       0      0      0 I   0.3   0.0   0:00.81 kworker/u263:2-events_unbound
		  28694 root      20   0       0      0      0 I   0.3   0.0   0:01.11 kworker/u264:5-events_power_efficient
		      1 root      20   0  166608  11404   8084 S   0.0   0.0   0:04.71 systemd
		      2 root      20   0       0      0      0 S   0.0   0.0   0:00.15 kthreadd
		      3 root      20   0       0      0      0 S   0.0   0.0   0:00.00 pool_workqueue_release

	Test
